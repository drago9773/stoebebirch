{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Demographic Data\n",
    "demographic_areas = gpd.read_file(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\census_block_group_source_nationwide\\v107\\blkgrp.gdb\")\n",
    "cbsa_source = gpd.read_file(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\cbsa_source\\tl_2020_us_cbsa.shp\")\n",
    "state_source = gpd.read_file(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\state_source\\States_shapefile.shp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_zips(state, city=None, zip_code=None):\n",
    "    zips = pd.read_csv(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\zipcode_source\\zip_code_database.csv\")\n",
    "    \n",
    "    if city is None and zip_code is None:\n",
    "        target_zips = zips[zips[\"state\"] == state][\"zip\"].tolist()\n",
    "    elif zip_code is None:\n",
    "        target_zips = zips[(zips[\"primary_city\"] == city) & (zips[\"state\"] == state)][\"zip\"].tolist()\n",
    "    else:\n",
    "        target_zips = [zip_code]\n",
    "    \n",
    "    return target_zips\n",
    "\n",
    "\n",
    "def get_stingray_rgn_id(zip):\n",
    "    query_location_api = f\"https://www.redfin.com/stingray/do/query-location?location={zip}&v=2\"\n",
    "    response = requests.get(query_location_api, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}) \n",
    "    soup = BeautifulSoup(response.text, 'html.parser').text\n",
    "    prefix_removed = soup.split('&&', 1)[1]\n",
    "    data = json.loads(prefix_removed)\n",
    "    try:\n",
    "        region_id = data[\"payload\"][\"exactMatch\"].get(\"id\").split(\"_\",1)[1]\n",
    "        return region_id\n",
    "    except:\n",
    "        # print(f\"No Exact match found for zip: {zip}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_stingray_gis_params(params):\n",
    "        return \"&\".join(f\"{key}={value}\" for key, value in params.items() if params.get(key) != None)\n",
    "\n",
    "\n",
    "def call_stingray_buy_gis(params_url):\n",
    "    api_url = \"https://www.redfin.com/stingray/api/gis\"\n",
    "    url = f\"{api_url}?{params_url}\"\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser').text\n",
    "    prefix_removed = soup.split('&&', 1)[1]\n",
    "    # print(url)\n",
    "    data = json.loads(prefix_removed)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_stingray_buy_gis(data):\n",
    "    homes = data.get('payload', {}).get('homes', [])\n",
    "    parsed_homes = []\n",
    "    \n",
    "    for home in homes:\n",
    "        lat_long = home.get('latLong', {}).get('value', {})\n",
    "        home_info = {\n",
    "            \"MLS ID\": home.get('mlsId', {}).get('value'),\n",
    "            \"Status\": home.get('mlsStatus'),\n",
    "            \"Price\": home.get('price', {}).get('value'),\n",
    "            \"HOA Fee\": home.get('hoa', {}).get('value'),\n",
    "            \"Square Feet\": home.get('sqFt', {}).get('value'),\n",
    "            \"Price per Square Foot\": home.get('pricePerSqFt', {}).get('value'),\n",
    "            \"Lot Size\": home.get('lotSize', {}).get('value'),\n",
    "            \"Bedrooms\": home.get('beds'),\n",
    "            \"Bathrooms\": home.get('baths'),\n",
    "            \"Location\": home.get('location', {}).get('value'),\n",
    "            \"Stories\": home.get('stories'),\n",
    "            \"Address\": home.get('streetLine', {}).get('value'),\n",
    "            \"City\": home.get('city'),\n",
    "            \"State\": home.get('state'),\n",
    "            \"ZIP Code\": home.get('postalCode', {}).get('value'),\n",
    "            \"Year Built\": home.get('yearBuilt', {}).get('value'),\n",
    "            \"URL\": home.get('url'),\n",
    "            \"Latitude\": lat_long.get('latitude'),\n",
    "            \"Longitude\": lat_long.get('longitude')\n",
    "        }\n",
    "        parsed_homes.append(home_info)\n",
    "    \n",
    "    return parsed_homes\n",
    "\n",
    "\n",
    "\n",
    "def geocode_dataframe(df, latitude_col='Latitude', longitude_col='Longitude', demographics_df = demographic_areas, cbsa_df = cbsa_source, state_df = state_source):\n",
    "    import geopandas as gpd\n",
    "    import pandas as pd\n",
    "    \"\"\"\n",
    "    Geocode the given DataFrame based on geographic data files.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the data to be geocoded.\n",
    "    longitude_col (str): Name of the column containing longitude values.\n",
    "    latitude_col (str): Name of the column containing latitude values.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Geocoded DataFrame.\n",
    "    \"\"\"\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, geometry=gpd.points_from_xy(df[longitude_col], df[latitude_col]), crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Load and preprocess demographic areas\n",
    "    demographic_areas = demographics_df\n",
    "    demographic_areas.to_crs(\"EPSG:4326\", inplace=True)\n",
    "    demographic_areas[\"GEOID\"] = demographic_areas[\"FIPS\"]\n",
    "    demographic_areas = demographic_areas[[\"GEOID\", \"geometry\"]].rename(columns={\"GEOID\": \"cbg_geoid\"})\n",
    "\n",
    "    # Load and preprocess CBSA areas\n",
    "    cbsa_source = cbsa_df\n",
    "    cbsa_source.to_crs(\"EPSG:4326\", inplace=True)\n",
    "    cbsa_source = cbsa_source[[\"GEOID\", \"NAME\", \"geometry\"]].rename(columns={\"GEOID\": \"cbsa_geoid\", \"NAME\": \"cbsa_name\"})\n",
    "\n",
    "    # Load and preprocess state areas\n",
    "    state_source = state_df\n",
    "    state_source.to_crs(\"EPSG:4326\", inplace=True)\n",
    "    state_source = state_source[[\"FID\", \"State_Code\", \"geometry\"]].rename(columns={\"FID\": \"state_id\", \"State_Name\": \"state_name\"})\n",
    "\n",
    "    # Perform spatial joins\n",
    "    geocoded_dots = gdf.sjoin(demographic_areas, how=\"left\").drop([\"index_right\"], axis=1)\n",
    "    geocoded_dots = geocoded_dots.sjoin(cbsa_source, how='left').drop([\"index_right\"], axis=1)\n",
    "    geocoded_dots = geocoded_dots.sjoin(state_source, how='left').drop([\"index_right\"], axis=1)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    geocoded_dots = geocoded_dots.drop(['geometry'], axis=1)\n",
    "\n",
    "    return pd.DataFrame(geocoded_dots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Zipcodes to be Scrubbed in TX: 2661\n",
      "0 Zip Codes Evaluated\n",
      "100 Zip Codes Evaluated\n",
      "200 Zip Codes Evaluated\n",
      "300 Zip Codes Evaluated\n",
      "400 Zip Codes Evaluated\n",
      "500 Zip Codes Evaluated\n",
      "600 Zip Codes Evaluated\n",
      "700 Zip Codes Evaluated\n",
      "800 Zip Codes Evaluated\n",
      "900 Zip Codes Evaluated\n",
      "1000 Zip Codes Evaluated\n",
      "1100 Zip Codes Evaluated\n",
      "1200 Zip Codes Evaluated\n",
      "1300 Zip Codes Evaluated\n",
      "1400 Zip Codes Evaluated\n",
      "1500 Zip Codes Evaluated\n",
      "1600 Zip Codes Evaluated\n",
      "1700 Zip Codes Evaluated\n",
      "1800 Zip Codes Evaluated\n",
      "1900 Zip Codes Evaluated\n",
      "2000 Zip Codes Evaluated\n",
      "2100 Zip Codes Evaluated\n",
      "2200 Zip Codes Evaluated\n",
      "2300 Zip Codes Evaluated\n",
      "2400 Zip Codes Evaluated\n",
      "2500 Zip Codes Evaluated\n",
      "2600 Zip Codes Evaluated\n",
      "here\n",
      "In the State of TX, 180368 Homes were Downloaded. 1380 were duplicates leaving 293236 homes in the whole dataset\n"
     ]
    }
   ],
   "source": [
    "# GIS Search API\n",
    "\n",
    "import requests\n",
    "\n",
    "Zip = None\n",
    "City = None\n",
    "States = [\"TX\"]\n",
    "\n",
    "\n",
    "for State in States:\n",
    "    target_zips = get_target_zips(State, City, Zip)\n",
    "    \n",
    "    print(f\"Number of Zipcodes to be Scrubbed in {State}: {len(target_zips)}\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for index, zip in enumerate(target_zips):\n",
    "        if index % 100 == 0:\n",
    "            print(f\"{index} Zip Codes Evaluated\")\n",
    "\n",
    "\n",
    "        params = {\n",
    "        #??Active Listings\n",
    "        \"al\": 1,\n",
    "        #Include Nearby Homes\n",
    "        \"include_nearby_homes\": \"false\",\n",
    "        # Market. ie Seattle\n",
    "        \"market\": None,\n",
    "        # Number of homes to retrieve\n",
    "        \"num_homes\": 350,\n",
    "        #How to Sort the homes\n",
    "        \"ord\": \"days-on-redfin-asc\",\n",
    "        \"page_number\": 1,\n",
    "        \"poly\": None,\n",
    "        #Listing Types\n",
    "        \"sf\": \"1,2,3,4,5,6,7\",\n",
    "        \"start\": None,\n",
    "        \"status\": 9,\n",
    "        # User input property types (currently only single family, townhomes : 13)\n",
    "        \"uipt\": \"1,3\",\n",
    "        # ??API Version?\n",
    "        \"v\": 8,\n",
    "        \"zoomLevel\": None,\n",
    "        #Type of Region analyzed\n",
    "        \"region_type\" : 2,\n",
    "        \"region_id\" : get_stingray_rgn_id(zip)\n",
    "        }\n",
    "\n",
    "        if params.get(\"region_id\") == None:\n",
    "            continue\n",
    "        else:\n",
    "            url_param = build_stingray_gis_params(params)\n",
    "            json_data = call_stingray_buy_gis(url_param)\n",
    "            list_data = parse_stingray_buy_gis(json_data)\n",
    "            \n",
    "            data.extend(list_data)\n",
    "\n",
    "    df= pd.DataFrame(data)\n",
    "    \n",
    "    df.drop_duplicates(subset=[\"MLS ID\"], inplace=True)\n",
    "\n",
    "    df[\"updated_date\"] = datetime.now().date()\n",
    "    print(\"here\")\n",
    "    df = geocode_dataframe(df, \"Latitude\", 'Longitude')\n",
    "\n",
    "    existing_homes = pd.read_csv(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Redfin Data\\for_sale_homes.csv\", index_col = None)\n",
    "\n",
    "    existing_homes['MLS ID'] = existing_homes['MLS ID'].astype(str).str.strip()\n",
    "    df['MLS ID'] = df['MLS ID'].astype(str).str.strip()\n",
    "\n",
    "    # Identify common Property IDs\n",
    "    common_property_ids = existing_homes[existing_homes['MLS ID'].isin(df['MLS ID'])]\n",
    "\n",
    "    # Filter out these common Property IDs from the existing rentals DataFrame\n",
    "    existing_homes = existing_homes[~existing_homes['MLS ID'].isin(common_property_ids['MLS ID'])]\n",
    "\n",
    "    updated_homes = pd.concat([existing_homes, df], ignore_index=True)\n",
    "\n",
    "    print(f\"In the State of {State}, {df.shape[0]} Homes were Downloaded. {common_property_ids.shape[0]} were duplicates leaving {updated_homes.shape[0]} homes in the whole dataset\")\n",
    "\n",
    "    updated_homes.to_csv(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Redfin Data\\for_sale_homes.csv\", index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
