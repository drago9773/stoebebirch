{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load Demographic Data\n",
    "demographic_areas = gpd.read_file(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\census_block_group_source_nationwide\\v107\\blkgrp.gdb\")\n",
    "cbsa_source = gpd.read_file(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\cbsa_source\\tl_2020_us_cbsa.shp\")\n",
    "state_source = gpd.read_file(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\state_source\\States_shapefile.shp\")\n",
    "\n",
    "\n",
    "def get_target_zips(state, city=None, zip_code=None):\n",
    "    zips = pd.read_csv(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\zipcode_source\\zip_code_database.csv\")\n",
    "    \n",
    "    if city is None and zip_code is None:\n",
    "        target_zips = zips[zips[\"state\"] == state][\"zip\"].tolist()\n",
    "    elif zip_code is None:\n",
    "        target_zips = zips[(zips[\"primary_city\"] == city) & (zips[\"state\"] == state)][\"zip\"].tolist()\n",
    "    else:\n",
    "        target_zips = [zip_code]\n",
    "    \n",
    "    return target_zips\n",
    "\n",
    "\n",
    "def get_stingray_rgn_id(zip):\n",
    "    query_location_api = f\"https://www.redfin.com/stingray/do/query-location?location={zip}&v=2\"\n",
    "    response = requests.get(query_location_api, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}) \n",
    "    soup = BeautifulSoup(response.text, 'html.parser').text\n",
    "    prefix_removed = soup.split('&&', 1)[1]\n",
    "    data = json.loads(prefix_removed)\n",
    "    try:\n",
    "        region_id = data[\"payload\"][\"exactMatch\"].get(\"id\").split(\"_\",1)[1]\n",
    "        return region_id\n",
    "    except:\n",
    "        print(f\"No Exact match found for zip: {zip}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_stingray_gis_params(params):\n",
    "        return \"&\".join(f\"{key}={value}\" for key, value in params.items() if params.get(key) != None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def call_stingray_rent_gis(params_url):\n",
    "    api_url = \"https://www.redfin.com/stingray/api/v1/search/rentals?\"\n",
    "    url = f\"{api_url}?{params_url}\"\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser').text\n",
    "    # print(url)\n",
    "    data = json.loads(soup)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_stingray_rent_gis(data):\n",
    "    homes = data.get('homes', [])\n",
    "    parsed_homes = []\n",
    "    \n",
    "    for home in homes:\n",
    "        home_data = home.get('homeData', {})\n",
    "        rental_data = home.get('rentalExtension', {})\n",
    "        \n",
    "        home_info = {\n",
    "            \"Property ID\": home_data.get('propertyId'),\n",
    "            \"URL\": home_data.get('url'),\n",
    "            \"Property Type\": home_data.get('propertyType'),\n",
    "            # \"Photos Info\": home_data.get('photosInfo', {}).get('photoRanges'),\n",
    "            # \"Static Map URL\": home_data.get('staticMapUrl'),\n",
    "            # \"Has AT&T Fiber\": home_data.get('hasAttFiber'),\n",
    "            \"Address\": home_data.get('addressInfo', {}).get('formattedStreetLine'),\n",
    "            \"City\": home_data.get('addressInfo', {}).get('city'),\n",
    "            \"State\": home_data.get('addressInfo', {}).get('state'),\n",
    "            \"ZIP Code\": home_data.get('addressInfo', {}).get('zip'),\n",
    "            \"Country Code\": home_data.get('addressInfo', {}).get('countryCode'),\n",
    "            \"Latitude\": home_data.get('addressInfo', {}).get('centroid', {}).get('centroid', {}).get('latitude'),\n",
    "            \"Longitude\": home_data.get('addressInfo', {}).get('centroid', {}).get('centroid', {}).get('longitude'),\n",
    "            \"Rental ID\": rental_data.get('rentalId'),\n",
    "            \"Max Beds\": rental_data.get('bedRange', {}).get('max'),\n",
    "            \"Max Baths\": rental_data.get('bathRange', {}).get('max'),\n",
    "            \"Max Square Feet\": rental_data.get('sqftRange', {}).get('max'),\n",
    "            \"Max Rent Price\": rental_data.get('rentPriceRange', {}).get('max'),\n",
    "            # \"Last Updated\": rental_data.get('lastUpdated'),\n",
    "            # \"Number of Available Units\": rental_data.get('numAvailableUnits'),\n",
    "            # \"Status\": rental_data.get('status'),\n",
    "            # \"Date Available\": rental_data.get('dateAvailable'),\n",
    "            # \"Rental Details Page Type\": rental_data.get('rentalDetailsPageType'),\n",
    "            # \"Search Rank Score\": rental_data.get('searchRankScore'),\n",
    "            # \"Freshness Timestamp\": rental_data.get('freshnessTimestamp'),\n",
    "            \"Description\": rental_data.get('description'),\n",
    "            # \"Revenue Per Lead\": rental_data.get('revenuePerLead'),\n",
    "            # \"Feed Source Internal ID\": rental_data.get('feedSourceInternalId'),\n",
    "            # \"Is Commercial Paid\": rental_data.get('isCommercialPaid'),\n",
    "            # \"Feed Original Source\": rental_data.get('feedOriginalSource'),\n",
    "            # \"Desktop Phone\": rental_data.get('desktopPhone'),\n",
    "            # \"Mobile Web Phone\": rental_data.get('mobileWebPhone'),\n",
    "            # \"Mobile App Phone\": rental_data.get('mobileAppPhone')\n",
    "        }\n",
    "        parsed_homes.append(home_info)\n",
    "    \n",
    "    return parsed_homes\n",
    "\n",
    "\n",
    "def geocode_dataframe(df, latitude_col='Latitude', longitude_col='Longitude', demographics_df = demographic_areas, cbsa_df = cbsa_source, state_df = state_source):\n",
    "    import geopandas as gpd\n",
    "    import pandas as pd\n",
    "    \"\"\"\n",
    "    Geocode the given DataFrame based on geographic data files.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the data to be geocoded.\n",
    "    longitude_col (str): Name of the column containing longitude values.\n",
    "    latitude_col (str): Name of the column containing latitude values.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Geocoded DataFrame.\n",
    "    \"\"\"\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, geometry=gpd.points_from_xy(df[longitude_col], df[latitude_col]), crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    demographic_areas = demographics_df\n",
    "\n",
    "    # Load and preprocess demographic areas\n",
    "    demographic_areas = demographics_df\n",
    "    demographic_areas.to_crs(\"EPSG:4326\", inplace=True)\n",
    "    demographic_areas[\"GEOID\"] = demographic_areas[\"FIPS\"]\n",
    "    demographic_areas = demographic_areas[[\"GEOID\", \"geometry\"]].rename(columns={\"GEOID\": \"cbg_geoid\"})\n",
    "\n",
    "    # Load and preprocess CBSA areas\n",
    "    cbsa_source = cbsa_df\n",
    "    cbsa_source.to_crs(\"EPSG:4326\", inplace=True)\n",
    "    cbsa_source = cbsa_source[[\"GEOID\", \"NAME\", \"geometry\"]].rename(columns={\"GEOID\": \"cbsa_geoid\", \"NAME\": \"cbsa_name\"})\n",
    "\n",
    "    # Load and preprocess state areas\n",
    "    state_source = state_df\n",
    "    state_source.to_crs(\"EPSG:4326\", inplace=True)\n",
    "    state_source = state_source[[\"FID\", \"State_Code\", \"geometry\"]].rename(columns={\"FID\": \"state_id\", \"State_Name\": \"state_name\"})\n",
    "\n",
    "    # Perform spatial joins\n",
    "    geocoded_dots = gdf.sjoin(demographic_areas, how=\"left\").drop([\"index_right\"], axis=1)\n",
    "    geocoded_dots = geocoded_dots.sjoin(cbsa_source, how='left').drop([\"index_right\"], axis=1)\n",
    "    geocoded_dots = geocoded_dots.sjoin(state_source, how='left').drop([\"index_right\"], axis=1)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    geocoded_dots = geocoded_dots.drop(['geometry'], axis=1)\n",
    "\n",
    "    return pd.DataFrame(geocoded_dots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Load Spatial Datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m demographic_areas \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mread_file(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmattl\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mreibrowser\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDatabase\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAreas\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcensus_block_group_source_nationwide\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mv107\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mblkgrp.gdb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m demographic_areas\u001b[38;5;241m.\u001b[39mto_crs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG:4326\u001b[39m\u001b[38;5;124m\"\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m demographic_areas[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEOID\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m      6\u001b[0m     demographic_areas[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTATE_FIPS\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m2\u001b[39m)  \\\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m+\u001b[39m demographic_areas[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOUNTY_FIPS\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m3\u001b[39m) \\\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m+\u001b[39m demographic_areas[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRACT_FIPS\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m6\u001b[39m) \\\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m+\u001b[39m demographic_areas[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLOCKGROUP_FIPS\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mattl\\anaconda3\\Lib\\site-packages\\geopandas\\io\\file.py:289\u001b[0m, in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m         path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_file_fiona(\n\u001b[0;32m    290\u001b[0m         path_or_bytes, from_bytes, bbox\u001b[38;5;241m=\u001b[39mbbox, mask\u001b[38;5;241m=\u001b[39mmask, rows\u001b[38;5;241m=\u001b[39mrows, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown engine \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mattl\\anaconda3\\Lib\\site-packages\\geopandas\\io\\file.py:372\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[1;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m    369\u001b[0m         [record[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m f_filt], columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    370\u001b[0m     )\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 372\u001b[0m     df \u001b[38;5;241m=\u001b[39m GeoDataFrame\u001b[38;5;241m.\u001b[39mfrom_features(\n\u001b[0;32m    373\u001b[0m         f_filt, crs\u001b[38;5;241m=\u001b[39mcrs, columns\u001b[38;5;241m=\u001b[39mcolumns \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    374\u001b[0m     )\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m datetime_fields:\n\u001b[0;32m    376\u001b[0m     as_dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mattl\\anaconda3\\Lib\\site-packages\\geopandas\\geodataframe.py:644\u001b[0m, in \u001b[0;36mGeoDataFrame.from_features\u001b[1;34m(cls, features, crs, columns)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(feature, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__geo_interface__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    642\u001b[0m     feature \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m.\u001b[39m__geo_interface__\n\u001b[0;32m    643\u001b[0m row \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 644\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m: shape(feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    645\u001b[0m }\n\u001b[0;32m    646\u001b[0m \u001b[38;5;66;03m# load properties\u001b[39;00m\n\u001b[0;32m    647\u001b[0m properties \u001b[38;5;241m=\u001b[39m feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\mattl\\anaconda3\\Lib\\site-packages\\shapely\\geometry\\geo.py:107\u001b[0m, in \u001b[0;36mshape\u001b[1;34m(context)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MultiLineString(ob[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m geom_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultipolygon\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MultiPolygon([[c[\u001b[38;5;241m0\u001b[39m], c[\u001b[38;5;241m1\u001b[39m:]] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m ob[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m geom_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometrycollection\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    109\u001b[0m     geoms \u001b[38;5;241m=\u001b[39m [shape(g) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m ob\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometries\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])]\n",
      "File \u001b[1;32mc:\\Users\\mattl\\anaconda3\\Lib\\site-packages\\shapely\\geometry\\multipolygon.py:84\u001b[0m, in \u001b[0;36mMultiPolygon.__new__\u001b[1;34m(self, polygons)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m         holes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     p \u001b[38;5;241m=\u001b[39m polygon\u001b[38;5;241m.\u001b[39mPolygon(shell, holes)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     p \u001b[38;5;241m=\u001b[39m polygon\u001b[38;5;241m.\u001b[39mPolygon(ob)\n",
      "File \u001b[1;32mc:\\Users\\mattl\\anaconda3\\Lib\\site-packages\\shapely\\geometry\\polygon.py:239\u001b[0m, in \u001b[0;36mPolygon.__new__\u001b[1;34m(self, shell, holes)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m         holes \u001b[38;5;241m=\u001b[39m [LinearRing(ring) \u001b[38;5;28;01mfor\u001b[39;00m ring \u001b[38;5;129;01min\u001b[39;00m holes]\n\u001b[1;32m--> 239\u001b[0m geom \u001b[38;5;241m=\u001b[39m shapely\u001b[38;5;241m.\u001b[39mpolygons(shell, holes\u001b[38;5;241m=\u001b[39mholes)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(geom, Polygon):\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid values passed to Polygon constructor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mattl\\anaconda3\\Lib\\site-packages\\shapely\\decorators.py:77\u001b[0m, in \u001b[0;36mmultithreading_enabled.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m array_args:\n\u001b[0;32m     76\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr, old_flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(array_args, old_flags):\n",
      "File \u001b[1;32mc:\\Users\\mattl\\anaconda3\\Lib\\site-packages\\shapely\\creation.py:265\u001b[0m, in \u001b[0;36mpolygons\u001b[1;34m(geometries, holes, indices, out, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(holes\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mnumber):\n\u001b[0;32m    263\u001b[0m         holes \u001b[38;5;241m=\u001b[39m linearrings(holes)\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mpolygons(geometries, holes, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Load Spatial Datasets\n",
    "demographic_areas = gpd.read_file(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\census_block_group_source_nationwide\\v107\\blkgrp.gdb\")\n",
    "demographic_areas.to_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "demographic_areas[\"GEOID\"] = \\\n",
    "    demographic_areas[\"STATE_FIPS\"].astype(str).str.zfill(2)  \\\n",
    "    + demographic_areas[\"COUNTY_FIPS\"].astype(str).str.zfill(3) \\\n",
    "    + demographic_areas[\"TRACT_FIPS\"].astype(str).str.zfill(6) \\\n",
    "    + demographic_areas[\"BLOCKGROUP_FIPS\"].astype(str)\n",
    "\n",
    "demographic_areas = demographic_areas[[\"GEOID\", \"geometry\"]].rename(columns={\"GEOID\":\"cbg_geoid\"})\n",
    "\n",
    "cbsa_source = gpd.read_file(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\cbsa_source\\tl_2020_us_cbsa.shp\")\n",
    "cbsa_source.to_crs(\"EPSG:4326\", inplace=True)\n",
    "cbsa_source = cbsa_source[[\"GEOID\", \"NAME\", \"geometry\"]].rename(columns={\"GEOID\":\"cbsa_geoid\", \"NAME\": \"cbsa_name\"})\n",
    "\n",
    "state_source = gpd.read_file(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\state_source\\States_shapefile.shp\")\n",
    "state_source.to_crs(\"EPSG:4326\", inplace=True)\n",
    "state_source = state_source[[\"FID\", \"State_Code\", \"geometry\"]].rename(columns={\"FID\":\"state_id\", \"State_Name\": \"state_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Zipcodes to be Scrubbed in TX: 10\n",
      "0 Zip Codes Evaluated\n",
      "475  Rentals Downloaded\n",
      "428  Duplicate Rentals\n",
      "55439  Total Rentals in Dataset\n",
      "Number of Zipcodes to be Scrubbed in WA: 10\n",
      "0 Zip Codes Evaluated\n",
      "209  Rentals Downloaded\n",
      "94  Duplicate Rentals\n",
      "55554  Total Rentals in Dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Zip = None\n",
    "City = None\n",
    "States = [\"TX\", \"WA\"]\n",
    "\n",
    "target_zips = []\n",
    "\n",
    "for State in States:\n",
    "    target_zips = get_target_zips(State, City, Zip)\n",
    "    \n",
    "    target_zips = target_zips[slice(10)]\n",
    "    print(f\"Number of Zipcodes to be Scrubbed in {State}: {len(target_zips)}\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for index, zip in enumerate(target_zips):\n",
    "        if index % 50 == 0:\n",
    "            print(f\"{index} Zip Codes Evaluated\")\n",
    "\n",
    "        params = {\n",
    "            #??Active Listings\n",
    "            \"al\": 1,\n",
    "            #Rentals Only\n",
    "            \"isRentals\":\"true\",\n",
    "            #Include Nearby Homes\n",
    "            \"include_nearby_homes\": \"false\",\n",
    "            # Market. ie Seattle\n",
    "            \"market\": None,\n",
    "            # Number of homes to retrieve\n",
    "            \"num_homes\": 350,\n",
    "            #How to Sort the homes\n",
    "            \"ord\": \"days-on-redfin-asc\",\n",
    "            \"page_number\": 1,\n",
    "            \"poly\": None,\n",
    "            #Listing Types\n",
    "            \"sf\": \"1,2,3,4,5,6,7\",\n",
    "            \"start\": None,\n",
    "            \"status\": 9,\n",
    "            # User input property types (currently only single family, townhomes, multifamily : 134)\n",
    "            \"uipt\": \"1,3,4\",\n",
    "            # ??API Version?\n",
    "            \"v\": 8,\n",
    "            \"zoomLevel\": None,\n",
    "            #Type of Region analyzed\n",
    "            \"region_type\" : 2,\n",
    "            \"region_id\" : get_stingray_rgn_id(zip)\n",
    "        }\n",
    "\n",
    "        if params.get(\"region_id\") == None:\n",
    "            continue\n",
    "        else:\n",
    "            url_param = build_stingray_gis_params(params)\n",
    "            json_data = call_stingray_rent_gis(url_param)\n",
    "            list_data = parse_stingray_rent_gis(json_data)\n",
    "\n",
    "            data.extend(list_data)\n",
    "\n",
    "    new_rentals = pd.DataFrame(data)\n",
    "    new_rentals.drop_duplicates(subset=[\"Property ID\"], inplace=True)\n",
    "\n",
    "    new_rentals[\"updated_date\"] = datetime.now().date()\n",
    "\n",
    "    new_rentals = geocode_dataframe(new_rentals, \"Latitude\", 'Longitude')\n",
    "    \n",
    "    existing_rentals = pd.read_csv(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Redfin Data\\rentals.csv\", index_col = None)\n",
    "\n",
    "\n",
    "\n",
    "    existing_rentals['Property ID'] = existing_rentals['Property ID'].astype(str).str.strip()\n",
    "    new_rentals['Property ID'] = new_rentals['Property ID'].astype(str).str.strip()\n",
    "\n",
    "    # Identify common Property IDs\n",
    "    common_property_ids = existing_rentals[existing_rentals['Property ID'].isin(new_rentals['Property ID'])]\n",
    "\n",
    "    # Filter out these common Property IDs from the existing rentals DataFrame\n",
    "    existing_rentals = existing_rentals[~existing_rentals['Property ID'].isin(common_property_ids['Property ID'])]\n",
    "\n",
    "    updated_rentals = pd.concat([existing_rentals, new_rentals], ignore_index=True)\n",
    "\n",
    "    print(new_rentals.shape[0], \" Rentals Downloaded\")\n",
    "    print(common_property_ids.shape[0], \" Duplicate Rentals\")\n",
    "    print(updated_rentals.shape[0], \" Total Rentals in Dataset\")\n",
    "\n",
    "    updated_rentals.to_csv(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Redfin Data\\rentals.csv\", index = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
