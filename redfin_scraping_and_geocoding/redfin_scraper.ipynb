{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from supabase import create_client, Client\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from redfin_scraping_utils import RentScraper, BuyScraper\n",
    "from geocoding_utils import Geocoder \n",
    "import geopandas as  gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "states = ['WA', 'ID', 'OR', 'MI', 'IL', 'IA', 'WI', 'MN', 'IN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Create Supabase Client\n",
    "url: str = os.getenv(\"SUPABASE_URL\")\n",
    "key: str = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "#Initialize Scraper\n",
    "buy_scraper = BuyScraper()\n",
    "rent_scraper = RentScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 733 Zip Codes in WA\n",
      "Processing 24% done (183/733 zip codes)\n",
      "Processing 49% done (366/733 zip codes)\n",
      "Processing 74% done (549/733 zip codes)\n",
      "Scraped 10834 for-rent listings for state WA\n",
      "Writing\n",
      "Write Complete\n",
      "Scraping 325 Zip Codes in ID\n",
      "Processing 24% done (81/325 zip codes)\n",
      "Processing 49% done (162/325 zip codes)\n",
      "Processing 74% done (243/325 zip codes)\n",
      "Scraped 1403 for-rent listings for state ID\n",
      "Writing\n",
      "Write Complete\n",
      "Scraping 492 Zip Codes in OR\n",
      "Processing 25% done (123/492 zip codes)\n",
      "Processing 50% done (246/492 zip codes)\n",
      "Processing 75% done (369/492 zip codes)\n",
      "Scraped 6371 for-rent listings for state OR\n",
      "Writing\n",
      "Write Complete\n",
      "Scraping 1170 Zip Codes in MI\n",
      "Processing 24% done (292/1170 zip codes)\n",
      "Processing 50% done (585/1170 zip codes)\n",
      "Processing 74% done (877/1170 zip codes)\n",
      "Scraped 6324 for-rent listings for state MI\n",
      "Writing\n",
      "Write Complete\n",
      "Scraping 1590 Zip Codes in IL\n",
      "Processing 24% done (397/1590 zip codes)\n",
      "Processing 50% done (795/1590 zip codes)\n",
      "Processing 74% done (1192/1590 zip codes)\n",
      "Scraped 21943 for-rent listings for state IL\n",
      "Writing\n",
      "Write Complete\n",
      "Scraping 1063 Zip Codes in IA\n",
      "Processing 24% done (265/1063 zip codes)\n",
      "Processing 49% done (531/1063 zip codes)\n",
      "Processing 74% done (797/1063 zip codes)\n",
      "Scraped 1757 for-rent listings for state IA\n",
      "Writing\n",
      "Write Complete\n",
      "Scraping 898 Zip Codes in WI\n",
      "Processing 24% done (224/898 zip codes)\n",
      "Processing 50% done (449/898 zip codes)\n",
      "Processing 74% done (673/898 zip codes)\n",
      "Scraped 6646 for-rent listings for state WI\n",
      "Writing\n",
      "Write Complete\n",
      "Scraping 1032 Zip Codes in MN\n",
      "Processing 25% done (258/1032 zip codes)\n",
      "Processing 50% done (516/1032 zip codes)\n",
      "Processing 75% done (774/1032 zip codes)\n",
      "Scraped 8224 for-rent listings for state MN\n",
      "Writing\n",
      "Write Complete\n",
      "Scraping 991 Zip Codes in IN\n",
      "Processing 24% done (247/991 zip codes)\n",
      "Processing 49% done (495/991 zip codes)\n",
      "Processing 74% done (743/991 zip codes)\n",
      "Scraped 5430 for-rent listings for state IN\n",
      "Writing\n",
      "Write Complete\n"
     ]
    }
   ],
   "source": [
    "states = ['WA', 'ID', 'OR', 'MI', 'IL', 'IA', 'WI', 'MN', 'IN']\n",
    "\n",
    "for state in states:\n",
    "    \n",
    "    #Initialize New DF for each run\n",
    "    rent_df = pd.DataFrame()\n",
    "    # Scrape for-sale listings and append to buy_df\n",
    "    rent_data = rent_scraper.scrape_state(state=state)\n",
    "    \n",
    "    if not rent_data.empty:\n",
    "        rent_df = pd.concat([rent_df, rent_data], ignore_index=True)\n",
    "    \n",
    "    rent_df = rent_df.replace(np.nan, None)\n",
    "    rent_df = rent_df.drop_duplicates(subset= \"property_id\")\n",
    "    records = rent_df.to_dict(orient='records')\n",
    "\n",
    "    print(f\"Scraped {len(rent_data)} for-rent listings for state {state}\")\n",
    "\n",
    "\n",
    "    geocoder = Geocoder(\n",
    "        rent_df, \n",
    "        latitude_col='latitude', \n",
    "        longitude_col='longitude'\n",
    "    )\n",
    "\n",
    "    df_geocoded = geocoder.geocode_all(\n",
    "        demographic_areas_path=r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\census_block_group_source_nationwide\\v107\\blkgrp.gdb\",\n",
    "        cbsa_source_path=r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\cbsa_source\\tl_2020_us_cbsa.shp\", \n",
    "        state_source_path=r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\state_source\\States_shapefile.shp\"\n",
    "    )\n",
    "\n",
    "    print(\"Writing\")\n",
    "\n",
    "    df_geocoded.to_csv(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Redfin Data\\rentals_0926.csv\", mode='a')\n",
    "\n",
    "    print(f\"Write Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 733 Zip Codes in WA\n",
      "Processing 24% done (183/733 zip codes)\n",
      "Processing 49% done (366/733 zip codes)\n",
      "Processing 74% done (549/733 zip codes)\n",
      "Scraped 37148 for-sale listings for state WA\n",
      "Writing...\n",
      "Write Complete\n",
      "Scraping 325 Zip Codes in ID\n",
      "Processing 24% done (81/325 zip codes)\n",
      "Processing 49% done (162/325 zip codes)\n",
      "Processing 74% done (243/325 zip codes)\n",
      "Scraped 16945 for-sale listings for state ID\n",
      "Writing...\n",
      "Write Complete\n",
      "Scraping 492 Zip Codes in OR\n",
      "Processing 25% done (123/492 zip codes)\n",
      "Processing 50% done (246/492 zip codes)\n",
      "Processing 75% done (369/492 zip codes)\n",
      "Scraped 27530 for-sale listings for state OR\n",
      "Writing...\n",
      "Write Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for state in states:\n",
    "    \n",
    "    #Initialize New DF for each run\n",
    "    buy_df = pd.DataFrame()\n",
    "    # Scrape for-sale listings and append to buy_df\n",
    "    buy_data = buy_scraper.scrape_state(state)\n",
    "    \n",
    "    if not buy_data.empty:\n",
    "        buy_df = pd.concat([buy_df, buy_data], ignore_index=True)\n",
    "    \n",
    "    buy_df = buy_df.replace(np.nan, None)\n",
    "    buy_df = buy_df.drop_duplicates(subset= \"property_id\")\n",
    "    records = buy_df.to_dict(orient='records')\n",
    "\n",
    "    print(f\"Scraped {len(buy_data)} for-sale listings for state {state}\")\n",
    "\n",
    "    geocoder = Geocoder(\n",
    "        buy_df, \n",
    "        latitude_col='latitude', \n",
    "        longitude_col='longitude'\n",
    "    )\n",
    "\n",
    "    df_geocoded = geocoder.geocode_all(\n",
    "        demographic_areas_path=r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\census_block_group_source_nationwide\\v107\\blkgrp.gdb\",\n",
    "        cbsa_source_path=r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\cbsa_source\\tl_2020_us_cbsa.shp\", \n",
    "        state_source_path=r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Areas\\state_source\\States_shapefile.shp\"\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # print(\"Writing...\")\n",
    "    # response = (\n",
    "    #     supabase.table(\"redfin_listings_bronze\")\n",
    "    #     .upsert(records, on_conflict=\"property_id\")\n",
    "    #     .execute()\n",
    "    # )\n",
    "\n",
    "    print(f\"Write Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattl\\AppData\\Local\\Temp\\ipykernel_26628\\3005869222.py:1: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Redfin Data\\geocoded_forsale.csv\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Redfin Data\\geocoded_forsale.csv\")\n",
    "\n",
    "\n",
    "\n",
    "with open(r'../train_predict\\utils.py') as f:\n",
    "    exec(f.read())\n",
    "\n",
    "# Load the selected features\n",
    "with open(r'../train_predict/selected_features.json', 'r') as f:\n",
    "    selected_features = json.load(f)\n",
    "\n",
    "\n",
    "df = df[(df[\"bedrooms\"] >= 1) \n",
    "        & (df[\"bedrooms\"] < 6)  \n",
    "        & (df[\"bathrooms\"] < 4)\n",
    "        & (df[\"bathrooms\"] >= 1)  \n",
    "        & (df[\"square_feet\"] < 5000)    \n",
    "        & (df[\"state\"].notna())]\n",
    "\n",
    "# Convert Bedrooms to string and clean up\n",
    "df[\"bedrooms\"] = df[\"bedrooms\"].astype(str).str.split('.').str[0].astype(int)\n",
    "\n",
    "#Convert bathrooms to .5 increments\n",
    "df['bathrooms'] = df['bathrooms'].round(1)\n",
    "df['bathrooms'] = (df['bathrooms'] * 2).round() / 2  # Ensures rounding to nearest 0.5\n",
    "\n",
    "\n",
    "basic_features = [\"square_feet\", \"bedrooms\", \"bathrooms\"]\n",
    "basic_metadata = ['mls_id', 'status', 'price', 'hoa_fee', 'lot_size', \n",
    "       'location', 'stories', 'address', 'city', 'state', 'zip_code',\n",
    "       'year_built', 'url', 'latitude', 'longitude', 'updated_date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_median_income_data(df, 'cbg_geoid', r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Rent Training Data\\ACSDT5Y2022.B19013-Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:201: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = fill_null(df, columns_to_fill=['median_income'], method='median', groupby='state')\n",
    "\n",
    "# Filter out rows where the state is not in the trained states\n",
    "states_trained = pd.read_csv(r\"C:\\Users\\mattl\\OneDrive\\Documents\\reibrowser\\Database\\Redfin Data\\rentals.csv\")\n",
    "states = states_trained[\"State_Code\"].unique()\n",
    "df = df[df[\"state_code\"].isin(states)]\n",
    "\n",
    "\n",
    "# Generate Rent Benchmarks using KNN models\n",
    "knn_features = [\"latitude\", \"longitude\"]\n",
    "n_values = [1, 5, 10]\n",
    "save_location = r'..\\Models'\n",
    "df, benchmark_features = create_knn_benchmark_rent(df, knn_features, target='Rent', n_values=n_values, save_location=save_location, mode='predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories [4.0] in column 1 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m columns_to_encode \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbedrooms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbathrooms\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m df, one_hot_features \u001b[38;5;241m=\u001b[39m one_hot_encode_features(\n\u001b[0;32m      3\u001b[0m     df, \n\u001b[0;32m      4\u001b[0m     columns_to_encode, \n\u001b[0;32m      5\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      6\u001b[0m     drop_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m      7\u001b[0m     encoder_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmattl\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProjects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mstoebebirch\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mModels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mone_hot_encoder.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      8\u001b[0m     feature_names_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain_predict\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mencoded_feature_names.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32m<string>:84\u001b[0m, in \u001b[0;36mone_hot_encode_features\u001b[1;34m(df, columns_to_encode, mode, drop_first, encoder_filename, feature_names_filename)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mattl\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mattl\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1016\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1015\u001b[0m }\n\u001b[1;32m-> 1016\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[0;32m   1017\u001b[0m     X,\n\u001b[0;32m   1018\u001b[0m     handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown,\n\u001b[0;32m   1019\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1020\u001b[0m     warn_on_unknown\u001b[38;5;241m=\u001b[39mwarn_on_unknown,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1023\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_idx_after_grouping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mattl\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:199\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    195\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unknown categories \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m in column \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m during transform\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(diff, i)\n\u001b[0;32m    198\u001b[0m     )\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m warn_on_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: Found unknown categories [4.0] in column 1 during transform"
     ]
    }
   ],
   "source": [
    "columns_to_encode = ['bedrooms', 'bathrooms']\n",
    "df, one_hot_features = one_hot_encode_features(\n",
    "    df, \n",
    "    columns_to_encode, \n",
    "    mode='predict', \n",
    "    drop_first=True, \n",
    "    encoder_filename=r'C:\\Users\\mattl\\OneDrive\\Desktop\\Projects\\stoebebirch\\Models\\one_hot_encoder.pkl', \n",
    "    feature_names_filename=r'..\\train_predict\\encoded_feature_names.json'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
